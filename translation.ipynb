{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YszrypEgvbVy"
      },
      "outputs": [],
      "source": [
        "!pip install -qq torch datasets transformers sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etDxuU47VfSj"
      },
      "source": [
        "# loading the dataset and inspecting it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHyRZGW7UcLY",
        "outputId": "d42cf55d-cd8d-437f-b5f4-a1f3e727154b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wmt17\", \"de-en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZugfOjcUzrf",
        "outputId": "1857f451-47af-4a0e-d6ab-4b527c55807f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.dataset_dict.DatasetDict'>\n"
          ]
        }
      ],
      "source": [
        "print(type(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImMxiUMjVd7T",
        "outputId": "e33942b9-55bd-4a2a-99a7-32b6d55ec689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys: dict_keys(['train', 'validation', 'test'])\n"
          ]
        }
      ],
      "source": [
        "print(\"keys:\", dataset.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmREy57LVwk0",
        "outputId": "a374cf1d-6ca5-4c65-fd9e-825498228367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation': {'de': 'Wiederaufnahme der Sitzungsperiode', 'en': 'Resumption of the session'}}\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSaW4nHcWHs0",
        "outputId": "26552bb5-ed4a-4d37-f8b1-983079dbb97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5906184\n",
            "2999\n",
            "3004\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset['train']))\n",
        "print(len(dataset['validation']))\n",
        "print(len(dataset['test']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yfNHwQquXh1r"
      },
      "outputs": [],
      "source": [
        "train = dataset['train'].shuffle(seed=1).select(range(10000))\n",
        "validation = dataset['validation']\n",
        "test = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjfdw2BOXyaD",
        "outputId": "d0b25b12-6657-474e-84ae-5cc072e6d01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n",
            "{'translation': {'de': 'Für die Erstellung und Verbreitung amtlicher Statistiken ist in Bulgarien das Nationale Institut für Statistik (National Statistics Institute, NSI) zuständig.', 'en': 'The National Statistics Institute (NSI) is the body charged with producing and disseminating official statistics in Bulgaria.'}}\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "print(type(train))\n",
        "print(train[0])\n",
        "print(len(train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UFWuncGX7pF"
      },
      "source": [
        "# tokenization:\n",
        "im using wordpiece\n",
        "\n",
        "bert base uncased adds cls and sep tokens. cls works as sos (start of sentence) and sep works as eos (end of sentence)\n",
        "\n",
        "maximum lenght is 128.\n",
        " more would be truncated, less would be padded and attention maks gets created\n",
        "when padding is done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q3OcJDPswolf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "MAXLEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uXHIHnnwxWC",
        "outputId": "214e2e03-888a-42b2-c9b5-230880d2ff0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train german ids: torch.Size([10000, 128])\n",
            "train english ids: torch.Size([10000, 128])\n",
            "train german mask: torch.Size([10000, 128])\n"
          ]
        }
      ],
      "source": [
        "def do_tokenize(texts):\n",
        "    return tok(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAXLEN,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# de\n",
        "train_de = [ex[\"translation\"][\"de\"] for ex in train]\n",
        "valid_de = [ex[\"translation\"][\"de\"] for ex in validation]\n",
        "test_de  = [ex[\"translation\"][\"de\"] for ex in test]\n",
        "\n",
        "train_de_tokens = do_tokenize(train_de)\n",
        "valid_de_tokens = do_tokenize(valid_de)\n",
        "test_de_tokens  = do_tokenize(test_de)\n",
        "\n",
        "# en\n",
        "train_en = [ex[\"translation\"][\"en\"] for ex in train]\n",
        "valid_en = [ex[\"translation\"][\"en\"] for ex in validation]\n",
        "test_en  = [ex[\"translation\"][\"en\"] for ex in test]\n",
        "\n",
        "train_en_tokens = do_tokenize(train_en)\n",
        "valid_en_tokens = do_tokenize(valid_en)\n",
        "test_en_tokens  = do_tokenize(test_en)\n",
        "\n",
        "# de masks (1=keep, 0=mask)\n",
        "train_de_mask = train_de_tokens[\"attention_mask\"]\n",
        "valid_de_mask = valid_de_tokens[\"attention_mask\"]\n",
        "test_de_mask  = test_de_tokens[\"attention_mask\"]\n",
        "\n",
        "print(\"train german ids:\", train_de_tokens[\"input_ids\"].shape)\n",
        "print(\"train english ids:\", train_en_tokens[\"input_ids\"].shape)\n",
        "print(\"train german mask:\", train_de_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCBvzXeCw9q5",
        "outputId": "dc140af9-7e8a-46bc-c2ab-e2a702b8f95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "de text: Für die Erstellung und Verbreitung amtlicher Statistiken ist in Bulgarien das Nationale Institut für Statistik (National Statistics Institute, NSI) zuständig.\n",
            "en text: The National Statistics Institute (NSI) is the body charged with producing and disseminating official statistics in Bulgaria.\n",
            "de token ids: tensor([  101,  6519,  3280,  9413, 13473,  3363,  5575,  6151, 12034,  2890,\n",
            "        28813,  2572, 19646, 17322,  2099, 28093,  2923, 17339,  2078, 21541,\n",
            "         1999, 20934, 27887, 23144,  8695, 17360, 17126,  6519, 28093,  2923,\n",
            "         5480,  1006,  2120,  6747,  2820,  1010, 24978,  2072,  1007, 16950,\n",
            "        21515,  8004,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "de mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "en token ids: tensor([  101,  1996,  2120,  6747,  2820,  1006, 24978,  2072,  1007,  2003,\n",
            "         1996,  2303,  5338,  2007,  5155,  1998,  4487, 11393, 27932,  2880,\n",
            "         6747,  1999,  8063,  1012,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "en mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "idx = 0\n",
        "print(\"de text:\", train[idx][\"translation\"][\"de\"])\n",
        "print(\"en text:\", train[idx][\"translation\"][\"en\"])\n",
        "print(\"de token ids:\", train_de_tokens[\"input_ids\"][idx])\n",
        "print(\"de mask:\", train_de_tokens[\"attention_mask\"][idx])\n",
        "print(\"en token ids:\", train_en_tokens[\"input_ids\"][idx])\n",
        "print(\"en mask:\", train_en_tokens[\"attention_mask\"][idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PuCJHI71jTg"
      },
      "source": [
        "# data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IZ5fHrG_1kZr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r0JVpyjn_H6a"
      },
      "outputs": [],
      "source": [
        "# make decoder input and labels (english side)\n",
        "dec_in = train_en_tokens[\"input_ids\"][:, :-1]  # everything except last token sep\n",
        "labels = train_en_tokens[\"input_ids\"][:, 1:].clone() # everything except first token cls\n",
        "val_dec_in = valid_en_tokens[\"input_ids\"][:, :-1]\n",
        "val_labels = valid_en_tokens[\"input_ids\"][:, 1:].clone()\n",
        "# change PAD (0) to -100 because i want loss to ignore it\n",
        "labels[labels == 0] = -100\n",
        "val_labels[val_labels == 0] = -100\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    train_de_tokens[\"input_ids\"],      # de sentences (to encoder)\n",
        "    train_de_tokens[\"attention_mask\"], # de mask (1 = real, 0 = pad)\n",
        "    dec_in,                            # en shifted input (to decoder)\n",
        "    labels                             # en shifted labels (for loss)\n",
        ")\n",
        "val_dataset = TensorDataset(\n",
        "    valid_de_tokens[\"input_ids\"],\n",
        "    valid_de_tokens[\"attention_mask\"],\n",
        "    val_dec_in,\n",
        "    val_labels\n",
        ")\n",
        "\n",
        "# make bathces using data loader:\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzKzLyYLDLoz"
      },
      "source": [
        "# trainig loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dP8sal43DNLV"
      },
      "outputs": [],
      "source": [
        "from transformers import EncoderDecoderModel\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFf-YETCE34h",
        "outputId": "4e19f1e0-992c-4778-d3c0-1b4e531ec3bd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:577: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/tmp/ipython-input-1534801198.py:45: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  total_loss += float(loss)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1: train_loss=5.4288  val_loss=5.2827\n",
            "epoch 2: train_loss=4.3931  val_loss=5.4375\n",
            "epoch 3: train_loss=3.7759  val_loss=5.5485\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    \"bert-base-uncased\",  # encoder\n",
        "    \"bert-base-uncased\",  # decoder\n",
        ")\n",
        "\n",
        "# special tokens : pad=0, cls=101, sep=102\n",
        "model.config.pad_token_id = tok.pad_token_id\n",
        "model.config.decoder_start_token_id = tok.cls_token_id\n",
        "model.config.eos_token_id = tok.sep_token_id\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def run_epoch(data_loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    for src_ids, src_mask, dec_in, labels in data_loader:\n",
        "        src_ids  = src_ids.to(device)\n",
        "        src_mask = src_mask.to(device)\n",
        "        dec_in   = dec_in.to(device)\n",
        "        labels   = labels.to(device)\n",
        "\n",
        "        # forward; model returns loss when labels are given\n",
        "        with torch.set_grad_enabled(train):\n",
        "            out = model(\n",
        "                input_ids=src_ids,\n",
        "                attention_mask=src_mask,\n",
        "                decoder_input_ids=dec_in,\n",
        "                labels=labels,             # has -100 for pads → ignored in loss\n",
        "            )\n",
        "            loss = out.loss\n",
        "\n",
        "            if train:\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                opt.step()\n",
        "\n",
        "        total_loss += float(loss)\n",
        "\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "EPOCHS = 3\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    train_loss = run_epoch(train_loader, train=True)\n",
        "    val_loss   = run_epoch(val_loader,   train=False)\n",
        "    print(f\"epoch {ep}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}